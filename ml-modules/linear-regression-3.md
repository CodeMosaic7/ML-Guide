# ðŸ“˜ Simple Linear Regression

## ðŸ§  1. Definition

**Simple Linear Regression (SLR)** is a **supervised learning algorithm** used to model the **relationship between two continuous variables**:

- **Independent Variable (X)** â€” Predictor or input  
- **Dependent Variable (Y)** â€” Response or output  

It assumes a **linear relationship** between X and Y, expressed as:

\[
Y = Î²_0 + Î²_1X + Îµ
\]

where:  
- \( Î²_0 \): Intercept (value of Y when X = 0)  
- \( Î²_1 \): Slope (change in Y for each unit change in X)  
- \( Îµ \): Error term (unexplained variation)

---

## ðŸŽ¯ 2. Goal

The goal of SLR is to find the **best-fitting line** through data points that **minimizes the difference** between the actual and predicted Y values.  
This best-fit line is called the **regression line**.

---

## âš™ï¸ 3. Working Principle

1. **Collect Data**  
   Gather paired data points \((X_i, Y_i)\).

2. **Assume Linear Model**  
   \[
   Y = Î²_0 + Î²_1X + Îµ
   \]

3. **Estimate Coefficients (Î²â‚€, Î²â‚)**  
   Using **Ordinary Least Squares (OLS)** â€” minimizes the **Sum of Squared Errors (SSE)**:
   \[
   SSE = \sum (Y_i - \hat{Y_i})^2
   \]

   Coefficients are computed as:
   \[
   Î²_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}
   \]
   \[
   Î²_0 = \bar{Y} - Î²_1\bar{X}
   \]

4. **Make Predictions**  
   \[
   \hat{Y} = Î²_0 + Î²_1X
   \]

---

## ðŸ“Š 4. Example

Predicting **exam score (Y)** based on **hours studied (X)**:

| Hours (X) | Score (Y) |
|------------|-----------|
| 1 | 45 |
| 2 | 50 |
| 3 | 55 |
| 4 | 60 |
| 5 | 65 |

Regression Line:
\[
\hat{Y} = 40 + 5X
\]

For 6 hours of study:  
\[
\text{Predicted Score} = 40 + 5(6) = \mathbf{70}
\]

---

## ðŸ“ 5. Assumptions of Simple Linear Regression

| Assumption | Description |
|-------------|--------------|
| **Linearity** | Relationship between X and Y is linear. |
| **Independence** | Observations are independent of each other. |
| **Homoscedasticity** | Constant variance of residuals across X values. |
| **Normality of Errors** | Residuals are normally distributed. |
| **No Multicollinearity** | Not applicable â€” only one predictor. |

---

## ðŸ“ 6. Evaluation Metrics

Used to evaluate model performance:

| Metric | Formula | Interpretation |
|---------|----------|----------------|
| **Mean Squared Error (MSE)** | \(\frac{1}{n}\sum (Y_i - \hat{Y_i})^2\) | Average squared difference between actual and predicted values. |
| **Root MSE (RMSE)** | \(\sqrt{MSE}\) | Error in the same units as Y. |
| **Mean Absolute Error (MAE)** | \(\frac{1}{n}\sum |Y_i - \hat{Y_i}|\) | Average absolute deviation from actual values. |
| **RÂ² (Coefficient of Determination)** | \(1 - \frac{SS_{res}}{SS_{tot}}\) | Proportion of variance in Y explained by X (0â€“1 scale). |

---

## ðŸ“‰ 7. Interpretation of Coefficients

- **Intercept (Î²â‚€):** Predicted value of Y when X = 0.  
- **Slope (Î²â‚):** Average change in Y for each 1-unit increase in X.  
  - If \( Î²_1 > 0 \): Positive relationship  
  - If \( Î²_1 < 0 \): Negative relationship

---

## ðŸ’» 8. Python Implementation

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Example Data
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
Y = np.array([45, 50, 55, 60, 65])

# Create Model
model = LinearRegression()
model.fit(X, Y)

# Predictions
Y_pred = model.predict(X)

# Coefficients
print("Intercept (Î²â‚€):", model.intercept_)
print("Slope (Î²â‚):", model.coef_[0])

# Visualization
plt.scatter(X, Y, color='blue', label='Actual Data')
plt.plot(X, Y_pred, color='red', label='Regression Line')
plt.xlabel("Hours Studied")
plt.ylabel("Exam Score")
plt.legend()
plt.title("Simple Linear Regression")
plt.show()
